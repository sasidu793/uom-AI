{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Dataset - Complete Data Cleaning and Analysis\n",
    "\n",
    "This notebook combines all data cleaning and analysis steps for the diabetes dataset:\n",
    "1. Initial data exploration (script.py)\n",
    "2. Data cleaning - handling missing values (script2.py)\n",
    "3. Outlier detection and removal (script3.py)\n",
    "4. Correlation analysis and visualization (chart_script.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the diabetes dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "print(\"Dataset successfully loaded!\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initial Data Exploration (from script.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning - Check for Missing Values (from script2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning - Check for missing values and data issues\n",
    "print(\"Data Cleaning Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for zero values that might indicate missing data\n",
    "print(\"\\nZero values in each column (potential missing data):\")\n",
    "zero_counts = (df == 0).sum()\n",
    "print(zero_counts)\n",
    "\n",
    "# Identify columns that shouldn't have zero values biologically\n",
    "problematic_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "print(\"\\nProblematic zero values (should not be zero biologically):\")\n",
    "for col in problematic_zeros:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    zero_percentage = (zero_count / len(df)) * 100\n",
    "    print(f\"{col}: {zero_count} zeros ({zero_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Replace Zeros with NaN and Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with NaN for problematic columns (except Insulin where 0 might be valid)\n",
    "df_cleaned = df.copy()\n",
    "for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']:\n",
    "    df_cleaned[col] = df_cleaned[col].replace(0, np.nan)\n",
    "\n",
    "print(\"After replacing zeros with NaN:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Replace missing values with median (more robust for outliers)\n",
    "for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']:\n",
    "    median_value = df_cleaned[col].median()\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(median_value)\n",
    "    print(f\"Filled {col} missing values with median: {median_value}\")\n",
    "\n",
    "print(\"\\nFinal missing values check:\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Outlier Detection (from script3.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection and Removal using IQR method\n",
    "print(\"Outlier Detection and Removal:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in each numerical column\n",
    "numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.drop('Outcome')\n",
    "outlier_info = {}\n",
    "\n",
    "print(\"Outlier analysis for each column:\")\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_cleaned, col)\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(df_cleaned)) * 100\n",
    "    outlier_info[col] = {\n",
    "        'count': outlier_count,\n",
    "        'percentage': outlier_percentage,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    print(f\"{col}: {outlier_count} outliers ({outlier_percentage:.2f}%), Range: [{lower:.2f}, {upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers (keeping moderate approach - only remove extreme outliers)\n",
    "df_no_outliers = df_cleaned.copy()\n",
    "initial_size = len(df_no_outliers)\n",
    "\n",
    "# Remove outliers for selected columns with high outlier counts\n",
    "cols_to_clean = ['Insulin', 'BMI', 'DiabetesPedigreeFunction']\n",
    "for col in cols_to_clean:\n",
    "    if outlier_info[col]['count'] > 0:\n",
    "        Q1 = df_no_outliers[col].quantile(0.25)\n",
    "        Q3 = df_no_outliers[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower_bound) & \n",
    "                                       (df_no_outliers[col] <= upper_bound)]\n",
    "\n",
    "final_size = len(df_no_outliers)\n",
    "removed_rows = initial_size - final_size\n",
    "print(f\"\\nRows removed due to outliers: {removed_rows} ({(removed_rows/initial_size)*100:.2f}%)\")\n",
    "print(f\"Final dataset size: {final_size}\")\n",
    "\n",
    "# Save cleaned data\n",
    "df_no_outliers.to_csv('diabetes_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned data saved as 'diabetes_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Correlation Analysis (from chart_script.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Column names:\", df.columns.tolist())\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Select only numerical columns for correlation\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(\"Correlation matrix shape:\", correlation_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Interactive Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',  # Diverging colormap (red-blue)\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    text=np.round(correlation_matrix.values, 2),  # Show correlation values\n",
    "    texttemplate='%{text}',\n",
    "    textfont={'size': 10},\n",
    "    showscale=True,\n",
    "    colorbar=dict(title='Correlation')\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Diabetes Dataset Correlation Matrix',\n",
    "    xaxis_title='Variables',\n",
    "    yaxis_title='Variables',\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Make sure the plot is square and readable\n",
    "fig.update_xaxes(side='bottom')\n",
    "fig.update_yaxes(autorange='reversed')\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the chart as both PNG and SVG\n",
    "fig.write_image('correlation_heatmap.png')\n",
    "fig.write_image('correlation_heatmap.svg', format='svg')\n",
    "\n",
    "print(\"Chart saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Alternative - Static Matplotlib Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a static heatmap using seaborn (alternative to plotly)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Diabetes Dataset Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook combined all the data cleaning and analysis steps:\n",
    "\n",
    "1. **Data Exploration**: Examined the dataset structure and basic statistics\n",
    "2. **Missing Values**: Identified zero values representing missing data\n",
    "3. **Data Imputation**: Replaced zeros with NaN and filled with median values\n",
    "4. **Outlier Detection**: Used IQR method to identify outliers\n",
    "5. **Outlier Removal**: Removed extreme outliers from selected columns\n",
    "6. **Correlation Analysis**: Created correlation matrix and visualizations\n",
    "7. **Export**: Saved cleaned dataset to 'diabetes_cleaned.csv'\n",
    "\n",
    "The cleaned dataset is now ready for machine learning modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
